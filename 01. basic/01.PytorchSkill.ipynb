{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor : '데이터를 표현하는 단위'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar : 상숫값 , 하나의 값을 표현할 때 1개의 수치로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "scalar1 = torch.tensor([1.])\n",
    "print(scalar1)\n",
    "\n",
    "scalar2 = torch.tensor([3.])\n",
    "print(scalar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([-2.])\n",
      "tensor([3.])\n",
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "add_scalar = scalar1 + scalar2\n",
    "print(add_scalar)\n",
    "\n",
    "\n",
    "sub_scalar = scalar1 - scalar2\n",
    "print(sub_scalar)\n",
    "\n",
    "mul_scalar = scalar1 * scalar2\n",
    "print(mul_scalar)\n",
    "\n",
    "div_scalar = scalar1 / scalar2\n",
    "print(div_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([-2.])\n",
      "tensor([3.])\n",
      "tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 모듈 활용하여 사칙연산 하기\n",
    "print(torch.add(scalar1,scalar2))\n",
    "print(torch.sub(scalar1,scalar2))\n",
    "print(torch.mul(scalar1,scalar2))\n",
    "print(torch.div(scalar1,scalar2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector : 하나의 값을 표현할 때 2개 이상의 수치로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "vector1 = torch.tensor([1.,2.,3.])\n",
    "print(vector1)\n",
    "\n",
    "vector2 = torch.tensor([4.,5.,6.])\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([-3., -3., -3.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "add_vector = vector1 + vector2\n",
    "print(add_vector)\n",
    "\n",
    "sub_vector = vector1 - vector2\n",
    "print(sub_vector)\n",
    "\n",
    "mul_vector = vector1 * vector2\n",
    "print(mul_vector)\n",
    "\n",
    "div_vector = vector1 / vector2\n",
    "print(div_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n",
      "tensor([-3., -3., -3.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 모듈 활용하여 vector 연산하기\n",
    "print(torch.add(vector1,vector2))\n",
    "print(torch.sub(vector1,vector2))\n",
    "print(torch.mul(vector1,vector2))\n",
    "print(torch.div(vector1,vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 : 2개 이상의 벡터 값을 통합해 구성된 값\n",
    "--> 벡터 값 간 연산 속도를 빠르게 진행할 수 있는 선형대수의 기본 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "matrix1 = torch.tensor([[1.,2.],[3.,4.]])\n",
    "print(matrix1)\n",
    "matrix2 = torch.tensor([[5.,6.],[7.,8.]])\n",
    "print(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "sum_matrix = matrix1 + matrix2\n",
    "print(sum_matrix)\n",
    "\n",
    "sub_matrix = matrix1 - matrix2\n",
    "print(sub_matrix)\n",
    "\n",
    "mul_matrix = matrix1 * matrix2\n",
    "print(mul_matrix)\n",
    "\n",
    "div_matrix = matrix1 / matrix2\n",
    "print(div_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 모듈 활용하여 matrix 연산하기\n",
    "print(torch.add(matrix1,matrix2))\n",
    "print(torch.sub(matrix1,matrix2))\n",
    "print(torch.mul(matrix1,matrix2))\n",
    "print(torch.div(matrix1,matrix2))\n",
    "\n",
    "\n",
    "# 행렬곱 연산\n",
    "print(torch.matmul(matrix1, matrix2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬을 2차원 배열이라 표현한다면, 텐서는 2차원 이상의 배열이라 표현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1.,2.],[3.,4.]],[[5.,6.],[7.,8.]]])\n",
    "print(tensor1)\n",
    "tensor2 = torch.tensor([[[9.,10.],[11.,12.]],[[13.,14.],[15.,16.]]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "sum_tensor = tensor1 + tensor2\n",
    "print(sum_tensor)\n",
    "\n",
    "sub_tensor = tensor1 - tensor2\n",
    "print(sub_tensor)\n",
    "\n",
    "mul_tensor = tensor1 * tensor2\n",
    "print(mul_tensor)\n",
    "\n",
    "div_tensor = tensor1 / tensor2\n",
    "print(div_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n",
      "tensor([[[ 31.,  34.],\n",
      "         [ 71.,  78.]],\n",
      "\n",
      "        [[155., 166.],\n",
      "         [211., 226.]]])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 모듈 활용하여 tensor 연산하기\n",
    "print(torch.add(tensor1,tensor2))\n",
    "print(torch.sub(tensor1,tensor2))\n",
    "print(torch.mul(tensor1,tensor2))\n",
    "print(torch.div(tensor1,tensor2))\n",
    "\n",
    "\n",
    "# 텐서곱 연산\n",
    "print(torch.matmul(tensor1, tensor2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치를 이용해 코드를 작성하면 back propagation을 이용해 파라미터를 업데이트한다.\n",
    "이는 Autograd 방식으로 쉽게 구현이 가능하도록 설정되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() : torch module을 이용할 때 GPU를 이용해 계산할 수 있는지 확인하는 method\n",
    "if torch.cuda.is_available() :\n",
    "    DEVICE = torch.device('cuda')\n",
    "else :\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BATCH_SIZE\n",
    "BATCH_SIZE : 파라미터를 업데이트할 때 계산되는 데이터의 개수\n",
    "BATCH_SIZE 수만큼 데이터를 이용해 output을 계산하고 BTACH_SIZE 수만큼 출력된 결과값에 대한 오차 계산\n",
    "BATCH_SIZE 수만큼 계산된 오차의 평균하여 Back propagation 적용하여 파라미터 업데이트한다.\n",
    "즉 , input으로 사용되는 데이터 개수가 64개라는 뜻이다.\n",
    "\n",
    "INPUT_SIZE\n",
    "input 크기이자, 입력층의 노드 수 --> 입력 데이터의 크기가 1000\n",
    "이는 (64,1000)으로 1000크기의 벡터 값을 64개 사용한다는 의미이다.\n",
    "\n",
    "HIDDEN_SIZE\n",
    "input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수를 의미한다.\n",
    "(64,1000)의 input들이 (1000,100) 크기의 행렬과 행렬 곱을 계산하기 위해 설정한 수이다.\n",
    "\n",
    "OUTPUT_SIZE\n",
    "최종으로 출력되는 값의 벡터의 크기 -> 최종으로 비교하고자 하는 레이블의 크기와 동일하게 설정\n",
    "ex) 10개로 분류한다면 크기가 10짜리인 one-hot encoding을 이용하기 위해 output 크기를 10으로 한다.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "BATCH_SIZE = 64 \n",
    "INPUT_SIZE = 1000 \n",
    "HIDDEN_SIZE = 100\n",
    "OUTPUT_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randn : 평균이 0, 표준편차가 1인 정규분포애서 샘플링한 값\n",
    "x = torch.randn(BATCH_SIZE, \n",
    "                INPUT_SIZE, \n",
    "                device = DEVICE, \n",
    "                dtype = torch.float, \n",
    "                requires_grad = False)\n",
    "\n",
    "y = torch.randn(BATCH_SIZE, \n",
    "                OUTPUT_SIZE, \n",
    "                device = DEVICE, \n",
    "                dtype = torch.float, \n",
    "                requires_grad = False)\n",
    "\n",
    "# requires_grad = True : gradient를 계산할 수 있도록 설정\n",
    "w1 = torch.randn(INPUT_SIZE, \n",
    "                HIDDEN_SIZE, \n",
    "                device = DEVICE, \n",
    "                dtype = torch.float, \n",
    "                requires_grad = True)\n",
    "\n",
    "# w2는 w1과 x를 행렬 곱한 결과에 계산할 수 있는 데이터\n",
    "# w1과 x의 행렬곱한 결과 : (1,100) \n",
    "# 따라서 (10,)output을 계산할 수 있도록 (100,10)행렬 이용\n",
    "w2 = torch.randn(HIDDEN_SIZE, \n",
    "                OUTPUT_SIZE, \n",
    "                device = DEVICE, \n",
    "                dtype = torch.float, \n",
    "                requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  100 \t Loss :  387.2408752441406\n",
      "Iteration :  200 \t Loss :  1.416290044784546\n",
      "Iteration :  300 \t Loss :  0.008046019822359085\n",
      "Iteration :  400 \t Loss :  0.00018531134992372245\n",
      "Iteration :  500 \t Loss :  3.5736218706006184e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-6 # gradient 값에 따른 학습 속도 결정\n",
    "                    # 파라미터 업데이트 시, gradient를 계산한 결과 값에 learning_rate만큼 곱한 값을 이용해 업데이트\n",
    "\n",
    "for t in range(1,501) :\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2) # x와 w1간의 헹렬 곱을 이용해 나온 값 계산\n",
    "                                          # clamp : torch 내 비선형 함수 적용 (ex. ReLU)\n",
    "                                          # 딥러닝 모델에서는 층과 층 사이에서 비선형 함수를 이용해 높은 표현력을 지니는 방정식을 얻는다.\n",
    "                                          # clamp로 나온 결과값과 w2의 행렬곱 수행\n",
    "        \n",
    "    \n",
    "    loss = (y_pred-y).pow(2).sum() # 오차계산\n",
    "    if t%100 == 0:\n",
    "        print(\"Iteration : \",t ,\"\\t\", \"Loss : \", loss.item())\n",
    "    loss.backward() # backward() : 각 파라미터에 대해 gradient를 계산하고, back propagation을 진행한다는 것을 의미 \n",
    "    \n",
    "    \n",
    "    # 각 파라미터 값에 대해 gradient 계산한 결과를 이용해 파라미터 값을 업데이트 할 때는ㄴ 해당 시점의 gradient 값을 고정한 후, 업데이트 한다.\n",
    "    with torch.no_grad(): \n",
    "        # 빼는 이유 : loss 값이 최소로 계산될 수 있는 파라미터 값을 찾ㅈ기 위해 gradient 값에 대한 반대 방향으로 계산한다는 것을 의미\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 파라미터 값을 업데이트한 후, gradient를 초기화해 다음 반복문을 수행할 수 있도록 gradient 값을 0으로 설정한다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study_pytorch",
   "language": "python",
   "name": "study_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
